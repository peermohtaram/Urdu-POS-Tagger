{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"urdu_kn_hmm_pos.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"E-rXSynAu_Rq","executionInfo":{"status":"ok","timestamp":1602185159846,"user_tz":-300,"elapsed":78721,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}},"outputId":"d082275c-23db-4ea7-ac42-b6b0c7e7ad36","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QBKipqqqxg45","executionInfo":{"status":"ok","timestamp":1602185165044,"user_tz":-300,"elapsed":3344,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["import collections\n","import itertools\n","import nltk\n","import math\n","import time\n","import sys\n","import ast\n","import numpy as np\n","from collections import defaultdict, deque\n","\n","START_SYMBOL = '*'\n","STOP_SYMBOL = 'STOP'\n","RARE_SYMBOL = '_RARE_'\n","RARE_WORD_MAX_FREQ = 1\n","LOG_PROB_OF_ZERO = -1000"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zj3XS55sUBop"},"source":["This function takes the input `training` data, make tuple of word and tag and the senteces based on full stop mark (.).\n","\n","**Output:** `[('اونٹ', 'NN'), ('کی', 'PSP'), ('سفاری', 'NN'), ('خاص', 'JJ'), ('پسند', 'NN'), ('ہو', 'VBF'), ('سکتی', 'AUXM'), ('ہے', 'AUXT'), ('۔', 'PU')]`"]},{"cell_type":"code","metadata":{"id":"2y39dN8tzIpr","executionInfo":{"status":"ok","timestamp":1602185165937,"user_tz":-300,"elapsed":991,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def data_convert(train_file):\n","  data = \"\"\n","\n","  for line in train_file:\n","      line = line.split(\"\\n\")[0]\n","      line = tuple(line.split(\"\\t\"))\n","      data+= str(line)+\", \"\n","  data =  data.replace(\"('۔', 'PU')\", \"('۔', 'PU')__\")\n","  data_converted = [\"[\"+ i.lstrip(\", \")+\"]\" for i in data.split(\"__\")][:-1]\n","  return data_converted"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0bY6G0hPeRA"},"source":["This function takes the training data and separates the words and tags and adds them to separate lists.\n","Each sentence is tokenized into its words and tags and appended into a list.\n","\n","**Sentence Format:** ```[['*', '*', 'w1', 'w2', 'STOP'], ['*', '*', 'w1', 'w2', 'STOP']]```\n","\n","**Tags Format:** `[['*', '*', 't1', 't2', 'STOP'], ['*', '*', 't1', 't2', 'STOP']]`\n","\n","Each sub-list shows a complete sentence. Double '*' and 'STOP' symbols are added in the start and end of a sentence respectively.\n"]},{"cell_type":"code","metadata":{"id":"I_EC6AiBvTAv","executionInfo":{"status":"ok","timestamp":1602185169611,"user_tz":-300,"elapsed":1756,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def wordstags_split(in_file):\n","  temp_list = in_file\n","  temp_sent = []\n","  temp_tags = []\n","  for each in temp_list:\n","    sentc, tags = zip(*ast.literal_eval(each))\n","    temp_sent.append([START_SYMBOL] * 2 + list(sentc) + [STOP_SYMBOL])\n","    temp_tags.append([START_SYMBOL] *2 + list(tags) + [STOP_SYMBOL])\n","  return temp_sent, temp_tags"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fT1vcL5zWqlp"},"source":["This function takes the `test data` as input and make senteces of test data, with each sentence on a single line."]},{"cell_type":"code","metadata":{"id":"q2-6JUdjuuhF","executionInfo":{"status":"ok","timestamp":1602185170768,"user_tz":-300,"elapsed":1236,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def make_test(test_file):\n","  data = \"\"\n","\n","  for line in test_file:\n","    line = line.split(\"\\n\")[0]\n","    data+= \" \"+str(line)\n","\n","  data =  data.replace(\"۔ \", \"۔__\").split('__')\n","  data_converted = [[w.strip() for w in each.split()] for each in data]\n","  return data_converted"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7DWqw2_OSA8a"},"source":["This function takes the tags list and make trigrams."]},{"cell_type":"code","metadata":{"id":"ygv93fYbwxd2","executionInfo":{"status":"ok","timestamp":1602185172041,"user_tz":-300,"elapsed":1353,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def calc_trigrams(urdu_tags):\n","    bigram_c = collections.defaultdict(int)\n","    trigram_c = collections.defaultdict(int)\n","\n","    for sentence in urdu_tags:\n","        for bigram in nltk.bigrams(sentence):\n","            bigram_c[bigram] += 1\n","\n","    for sentence in urdu_tags:\n","        for trigram in nltk.trigrams(sentence):\n","            trigram_c[trigram] += 1\n","\n","    return bigram_c, trigram_c"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xm6vz9SqW9Au"},"source":["This function implements the Kneser-Ney smoothing using built-in function in NLTK."]},{"cell_type":"code","metadata":{"id":"eJsdyOf6waP6","executionInfo":{"status":"ok","timestamp":1602185172647,"user_tz":-300,"elapsed":794,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def kneser_ney(tri_grams):\n","  freq_dist = nltk.probability.FreqDist([*tri_grams])\n","  for k, v in freq_dist.items():\n","    freq_dist[k] = tri_grams[k]\n","  KN = nltk.KneserNeyProbDist(freq_dist)\n","  KNDict = {}\n","  for i in KN.samples():\n","      KNDict[i] = KN.prob(i)\n","  return KNDict"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uelm3d3jXXK5"},"source":["This function takes the words from the training data and returns a python list of all of the words that occur more than value of `RARE_SYMBOL` parameter."]},{"cell_type":"code","metadata":{"id":"BSFGa8MjvUfF","executionInfo":{"status":"ok","timestamp":1602185175059,"user_tz":-300,"elapsed":1614,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def calc_known(urdu_words):\n","    known_words = set()\n","    word_c = defaultdict(int)\n","\n","    for sent_words in urdu_words:\n","        for word in sent_words:\n","            word_c[word] += 1\n","\n","    for word, count in word_c.items():\n","        if count > RARE_WORD_MAX_FREQ:\n","            known_words.add(word)\n","    return known_words"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLNaUs6XYJOn"},"source":["This function takes a set of sentences and a set of words that should not be marked `_RARE_`. Outputs a version of the set of sentences with rare words marked `_RARE_`"]},{"cell_type":"code","metadata":{"id":"t-ozuOuhvatx","executionInfo":{"status":"ok","timestamp":1602185176421,"user_tz":-300,"elapsed":1548,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def replace_rare(urdu_words, known_words):\n","    for i, sent_words in enumerate(urdu_words):\n","        for j, word in enumerate(sent_words):\n","            if word not in known_words:\n","                urdu_words[i][j] = RARE_SYMBOL\n","    return urdu_words"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxkl5FS-X9t6"},"source":["This function calculates emission probabilities and creates a list of possible tags.\n","The first return value is a python dictionary where each key is a tuple in which the first element is a word and the second is a tag and the value is the log probability of that word/tag pair and the second return value is a list of possible tags for this data set"]},{"cell_type":"code","metadata":{"id":"pZCf7fP_veUd","executionInfo":{"status":"ok","timestamp":1602185177145,"user_tz":-300,"elapsed":811,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def calc_emission(urdu_words_rare, urdu_tags):\n","    e_values = {}\n","    e_values_c = collections.defaultdict(int)\n","    tags_c = collections.defaultdict(int)\n","\n","    for word_sentence, tag_sentence in zip(urdu_words_rare, urdu_tags):\n","        for word, tag in zip(word_sentence, tag_sentence):\n","            e_values_c[(word, tag)] += 1\n","            tags_c[tag] += 1\n","\n","    for (word, tag), p in e_values_c.items():\n","        e_values[(word, tag)] = math.log(float(p) / tags_c[tag], 2)\n","\n","    return e_values, set(tags_c)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9UcFooX4Y2vO"},"source":["This function takes data to tag , possible tags (taglist), a list of known words (knownwords), trigram probabilities (qvalues) and emission probabilities (evalues) and outputs a list where every element is a string of a sentence tagged in the `WORD   TAG` format\n","\n","\n","`qvalues` is from the return of calc_trigrams = probability of the trigrams of tags\n","\n","`evalues` is from the return of calc_emission()\n","\n","Tagged is a list of tagged sentences in the format `WORD  TAG`. Each sentence is a string with a terminal newline, not a list of tokens."]},{"cell_type":"code","metadata":{"id":"iYK9whgqyeUG","executionInfo":{"status":"ok","timestamp":1602185182596,"user_tz":-300,"elapsed":2593,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def viterbi(urdu_dev_words, taglist, known_words, q_values, e_values):\n","    tagged = []\n","    pi = collections.defaultdict(float)\n","    bp = {}\n","    bp[(-1, START_SYMBOL, START_SYMBOL)] = START_SYMBOL\n","    pi[(-1, START_SYMBOL, START_SYMBOL)] = 0.0\n","\n","    for tokens_orig in urdu_dev_words:\n","        tokens = [w if w in known_words else RARE_SYMBOL for w in tokens_orig]\n","\n","        # k = 1 case\n","        for w in taglist:\n","            word_tag = (tokens[0], w)\n","            trigram = (START_SYMBOL, START_SYMBOL, w)\n","            pi[(0, START_SYMBOL, w)] = pi[(-1, START_SYMBOL, START_SYMBOL)] + q_values.get(trigram, LOG_PROB_OF_ZERO) + e_values.get(word_tag, LOG_PROB_OF_ZERO)\n","            bp[(0, START_SYMBOL, w)] = START_SYMBOL\n","\n","        # k = 2 case\n","        for w in taglist:\n","            for u in taglist:\n","                word_tag = (tokens[1], u)\n","                trigram = (START_SYMBOL, w, u)\n","                pi[(1, w, u)] = pi.get((0, START_SYMBOL, w), LOG_PROB_OF_ZERO) + q_values.get(trigram, LOG_PROB_OF_ZERO) + e_values.get(word_tag, LOG_PROB_OF_ZERO)\n","                bp[(1, w, u)] = START_SYMBOL\n","\n","        # k >= 2 case\n","        for k in range(2, len(tokens)):\n","            for u in taglist:\n","                for v in taglist:\n","                    max_prob = float('-Inf')\n","                    max_tag = ''\n","                    for w in taglist:\n","                        score = pi.get((k - 1, w, u), LOG_PROB_OF_ZERO) + q_values.get((w, u, v), LOG_PROB_OF_ZERO) + e_values.get((tokens[k], v), LOG_PROB_OF_ZERO)\n","                        if (score > max_prob):\n","                            max_prob = score\n","                            max_tag = w\n","                    bp[(k, u, v)] = max_tag\n","                    pi[(k, u, v)] = max_prob\n","\n","        max_prob = float('-Inf')\n","        v_max, u_max = None, None\n","        # finding the max probability of last two tags\n","        for (u, v) in itertools.product(taglist, taglist):\n","            score = pi.get((len(tokens_orig) - 1, u, v), LOG_PROB_OF_ZERO) + q_values.get((u, v, STOP_SYMBOL), LOG_PROB_OF_ZERO)\n","            if score > max_prob:\n","                max_prob = score\n","                u_max = u\n","                v_max = v\n","        # append tags in reverse order\n","        tags = []\n","        tags.append(v_max)\n","        tags.append(u_max)\n","\n","        for count, k in enumerate(range(len(tokens_orig) - 3, -1, -1)):\n","            tags.append(bp[(k + 2, tags[count + 1], tags[count])])\n","\n","        tagged_sentence = \"\"\n","        # reverse tags\n","        tags.reverse()\n","        # stringify tags paired with word without start and stop symbols\n","        for k in range(0, len(tokens_orig)):\n","            tagged.append(tagged_sentence + tokens_orig[k] + \"\\t\" + str(tags[k]))\n","\n","    return tagged"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11tWVn77Z3he"},"source":["This function writes the output of `viterbi` (tagged data) into a text file and save on the current/specified location."]},{"cell_type":"code","metadata":{"id":"Xm4j08y8v0DT","executionInfo":{"status":"ok","timestamp":1602185186159,"user_tz":-300,"elapsed":2437,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}}},"source":["def q5_output(tagged, filename):\n","    outfile = open(filename, 'w')\n","    for sentence in tagged:\n","        outfile.write(sentence)\n","        outfile.write('\\n')\n","    outfile.close()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Umqhxn5hv833","executionInfo":{"status":"ok","timestamp":1602186059904,"user_tz":-300,"elapsed":570223,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}},"outputId":"043b4182-3a96-41d2-d8a0-a67b78b0b1ba","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["def main():\n","\n","  OUTPUT_PATH = '/content/drive/My Drive/POSTagger/POSTaggerOut/'\n","\n","  time.clock()\n","\n","  infile = open('/content/drive/My Drive/POSTagger/Training_data.txt', 'r', encoding='utf-8-sig')\n","  train = infile.readlines()\n","  infile.close()\n","\n","  test_file = open(\"/content/drive/My Drive/POSTagger/Test_data.txt\", 'r')\n","  test = test_file.readlines()\n","  infile.close()\n","\n","  train_data = data_convert(train)\n","  urdu_words, urdu_tags = wordstags_split(train_data)\n","  test_words = make_test(test)\n","  print('Train and Test Files Read')\n","  print('Time: ' + str(time.clock()) + ' sec')\n","\n","  bigram_c, trigram_c = calc_trigrams(urdu_tags)\n","  q_values = kneser_ney(trigram_c)\n","  print('Smoothing Applied')\n","  print('Time: ' + str(time.clock()) + ' sec')\n","\n","  known_words = calc_known(urdu_words)\n","  urdu_words_rare = replace_rare(urdu_words, known_words)\n","\n","  e_values, taglist = calc_emission(urdu_words_rare, urdu_tags)\n","  print('e values computed')\n","\n","  # del urdu_train\n","  # del urdu_words_rare\n","\n","  print('Viterbi Starting')\n","  print('Time: ' + str(time.clock()) + ' sec')\n","\n","  viterbi_tagged = viterbi(test_words, taglist, known_words, q_values, e_values)\n","\n","  print('Viterbi Done')\n","  print('Time: ' + str(time.clock()) + ' sec')\n","\n","  q5_output(viterbi_tagged, OUTPUT_PATH + 'tagged_output.txt')\n","\n","  print('Time: ' + str(time.clock()) + ' sec')\n","\n","if __name__=='__main__':\n","    main()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Train and Test Files Read\n","Time: 185.8167 sec\n","Smoothing Applied\n","Time: 186.121314 sec\n","e values computed\n","Viterbi Starting\n","Time: 186.321003 sec\n","Viterbi Done\n","Time: 751.871323 sec\n","Time: 751.877684 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EKlohpXmaPTr"},"source":["This function takes the tagged output and correct tagged sentences and calculates the accuracy of tagged sentences."]},{"cell_type":"code","metadata":{"id":"w7HCO-p2Ly0N","executionInfo":{"status":"ok","timestamp":1602186093797,"user_tz":-300,"elapsed":1265,"user":{"displayName":"Muhammad Burhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHpWTqgpgNA5kLMlygCi2z-UZ-fNmS1TberFQF=s64","userId":"14916516780007559603"}},"outputId":"e92836f7-5810-472a-8a63-8bab28aa380f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import sys\n","import ast\n","\n","def main():\n","    if len(sys.argv) < 3:\n","        print( \"Usage: python POS-S.py <tagger output> <reference file>\")\n","        exit(1)\n","\n","    infile = open('/content/drive/My Drive/POSTagger/POSTaggerOut/tagged_output.txt', \"r\", encoding= 'utf-8-sig')\n","    user_sentences = infile.readlines()\n","    infile.close()\n","\n","    infile = open('/content/drive/My Drive/POSTagger/Validation-data.txt', \"r\", encoding= 'utf-8-sig')\n","    correct_sentences = infile.readlines()\n","    infile.close()\n","\n","    num_correct = 0\n","    total = 0\n","    for i in range(len(user_sentences)):\n","      user = user_sentences[i].split('\\n')[0]\n","      correct = correct_sentences[i].split('\\n')[0]\n","      if user == correct:\n","        num_correct += 1\n","      total += 1\n","        \n","    score = float(num_correct) / total * 100\n","\n","    print(\"Percent correct tags:\", score)\n","\n","if __name__=='__main__':\n","    main()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Percent correct tags: 88.57000000000001\n"],"name":"stdout"}]}]}